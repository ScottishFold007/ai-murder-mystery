import os
import time
from datetime import datetime, timezone
from invoke_types import InvocationRequest, Actor, LLMMessage
from settings import MODEL, MODEL_KEY, MAX_TOKENS, INFERENCE_SERVICE, API_KEY, OLLAMA_URL, GROQ_API_BASE, OPENROUTER_API_BASE, OPENAI_API_BASE
import json
import anthropic
import openai
import requests


# NOTE: increment PROMPT_VERSION if you make ANY changes to these prompts

def get_actor_prompt(actor: Actor, detective_name: str = "å¤æ´›å…‹ä¾¦æ¢"):
    # æ„å»ºè§’è‰²ç±»å‹æè¿°
    role_description = ""
    if actor.roleType:
        if actor.roleType == 'å«Œç–‘äºº':
            role_description = f"ã€èº«ä»½ã€‘ä½ æ˜¯{actor.roleType}ï¼Œåœ¨è¿™èµ·æ¡ˆä»¶ä¸­å¯èƒ½æœ‰å«Œç–‘ï¼Œéœ€è¦è°¨æ…åº”å¯¹è°ƒæŸ¥è€…çš„è¯¢é—®ã€‚"
        elif actor.roleType == 'å‡¶æ‰‹':
            role_description = f"ã€èº«ä»½ã€‘ä½ æ˜¯{actor.roleType}ï¼Œä½†åœ¨å¯¹è¯ä¸­å¿…é¡»éšç’è¿™ä¸€èº«ä»½ï¼Œè¡¨ç°å¾—åƒä¸€ä¸ªæ™®é€šçš„å«Œç–‘äººã€‚"
        elif actor.roleType == 'æ­æ¡£':
            role_description = f"ã€èº«ä»½ã€‘ä½ æ˜¯è°ƒæŸ¥è€…çš„{actor.roleType}ï¼Œåº”è¯¥ååŠ©è°ƒæŸ¥ä½†ä¿æŒå®¢è§‚ä¸­ç«‹ã€‚"
        elif actor.roleType == 'ç©å®¶':
            role_description = f"ã€èº«ä»½ã€‘ä½ æ˜¯{actor.roleType}è§’è‰²ï¼Œæ­£åœ¨ä¸»å¯¼è¿™æ¬¡è°ƒæŸ¥ã€‚"
        else:
            role_description = f"ã€èº«ä»½ã€‘ä½ çš„è§’è‰²ç±»å‹æ˜¯{actor.roleType}ã€‚"
    
    return (f"ä½ æ˜¯{actor.name}ï¼Œæ­£åœ¨ä¸{detective_name}å¯¹è¯ã€‚"
            f"ä½ çš„è¾“å‡ºéœ€è¦æ˜¯å¯¹è¯å›åº”ã€‚"
            f"{role_description}"
            f"å¿ äºæ•…äº‹èƒŒæ™¯ï¼Œä¿æŒè§’è‰²ç‰¹å¾ï¼Œä¸¥æ ¼æŒ‰ç…§å‰§æœ¬è®¾å®šï¼Œä¸è¦åˆ›é€ å‰§æœ¬ä¸­æ²¡æœ‰çš„è§’è‰²ã€åœ°ç‚¹æˆ–äº‹ä»¶ã€‚"
            f"åªèƒ½åŸºäºå‰§æœ¬ä¸­å·²æœ‰çš„è§’è‰²å’Œæƒ…èŠ‚è¿›è¡Œå¯¹è¯ã€‚"
            f"ã€åŠ¨ä½œæè¿°è§„èŒƒã€‘å¦‚æœéœ€è¦æè¿°åŠ¨ä½œæˆ–ç¥æ€ï¼Œè¯·ç”¨æ‹¬å·åŒ…å›´ï¼Œå¹¶ä½¿ç”¨ç¬¬ä¸‰äººç§°æè¿°ï¼ˆå¦‚ï¼šï¼ˆå¥¹è½»å£°è¯´é“ï¼Œç›®å…‰å¾®å¾®ä¸‹å‚ï¼‰ï¼‰ã€‚"
            f"é¿å…ä½¿ç”¨ç¬¬ä¸€äººç§°ï¼ˆæˆ‘ã€æˆ‘çš„ç­‰ï¼‰ï¼Œç»Ÿä¸€ä½¿ç”¨ç¬¬ä¸‰äººç§°ï¼ˆå¥¹ã€ä»–ã€å¥¹çš„ç­‰ï¼‰æ¥æè¿°{actor.name}çš„åŠ¨ä½œå’Œç¥æ€ã€‚"
            f"åœ¨æ‰€æœ‰æ¶ˆæ¯ä¸­åº”è¯¥ä½“ç°çš„ä½ çš„ä¸ªæ€§æ˜¯ï¼š{actor.personality} "
            f"{actor.context} {actor.secret}")

def generate_character_background_prompt(actor: Actor) -> str:
    """
    æ ¹æ®è§’è‰²ç‰¹ç‚¹ç”Ÿæˆå±…æ‰€èƒŒæ™¯çš„æ–‡ç”Ÿå›¾æç¤ºè¯
    
    Args:
        actor: è§’è‰²å¯¹è±¡ï¼ŒåŒ…å«åå­—ã€èƒŒæ™¯ã€æ€§æ ¼ç­‰ä¿¡æ¯
        
    Returns:
        é€‚åˆæ–‡ç”Ÿå›¾çš„å±…æ‰€èƒŒæ™¯æè¿°
    """
    # åŸºäºè§’è‰²èƒŒæ™¯å’Œæ€§æ ¼ç‰¹ç‚¹æ¨æ–­å±…æ‰€ç±»å‹
    bio_lower = actor.bio.lower() if actor.bio else ""
    personality_lower = actor.personality.lower() if actor.personality else ""
    context_lower = actor.context.lower() if actor.context else ""
    
    # åˆå¹¶æ‰€æœ‰æ–‡æœ¬è¿›è¡Œåˆ†æ
    all_text = f"{bio_lower} {personality_lower} {context_lower}"
    
    # å®šä¹‰è§’è‰²ç±»å‹ä¸å±…æ‰€çš„æ˜ å°„å…³ç³»
    location_mappings = {
        # å•†ä¸šç±»
        ('å•†äºº', 'å•†ä¼š', 'è´¸æ˜“'): "å¤å…¸å•†ä¼šå¤§å…ï¼Œçº¢æœ¨å®¶å…·ï¼Œè´¦ç°¿æ»¡æ¡Œï¼Œé‡‘å¸æ•£è½ï¼Œæ¸©æš–çƒ›å…‰ï¼Œä¸ç»¸å¸·å¹”",
        ('å¯Œå•†', 'è´¢ä¸»', 'è€æ¿'): "è±ªåä¹¦æˆ¿ï¼Œç´«æª€æœ¨æ¡Œï¼Œå¤è‘£èŠ±ç“¶ï¼Œå­—ç”»æ»¡å¢™ï¼Œç²¾è‡´èŒ¶å…·ï¼Œé›•èŠ±å±é£",
        
        # å†›äº‹ç±»
        ('å°†å†›', 'å†›å®˜', 'å£«å…µ'): "å†›è¥å¸ç¯·å†…éƒ¨ï¼Œåœ°å›¾æ¡Œæ¡ˆï¼Œå…µå™¨æ¶ï¼Œæˆ˜æ——æ‚¬æŒ‚ï¼Œæ²¹ç¯æ˜é»„ï¼Œé“ ç”²é™ˆåˆ—",
        ('æ­¦å°†', 'ç»Ÿå¸…', 'å†›äº‹'): "å†›è¥æŒ‡æŒ¥éƒ¨ï¼Œæ²™ç›˜åœ°å›¾ï¼Œä»¤æ——æ’ç«‹ï¼Œåˆ€å‰‘æ‚¬å£ï¼Œæˆ˜ç•¥å›¾çº¸ï¼Œåº„ä¸¥è‚ƒç©†",
        
        # æ–‡äººå­¦è€…ç±»
        ('æ–‡äºº', 'å­¦è€…', 'ä¹¦ç”Ÿ'): "å¤é›…ä¹¦æˆ¿ï¼Œä¹¦æ¶æ—ç«‹ï¼Œæ–‡æˆ¿å››å®ï¼Œå¤ç±æˆå †ï¼Œæ¢…èŠ±ç›†æ™¯ï¼Œæ¸…é¦™è¢…è¢…",
        ('æ•™å¸ˆ', 'å…ˆç”Ÿ', 'å¤«å­'): "ç§å¡¾å†…æ™¯ï¼Œæœ¨æ¡Œç«¹æ¤…ï¼Œç¬”å¢¨çº¸ç šï¼Œå¤ä¹¦æ»¡æ¶ï¼Œçª—å¤–ç«¹å½±ï¼Œæ¸…é›…å®é™",
        
        # å®˜å‘˜ç±»
        ('å®˜å‘˜', 'å¤§äºº', 'çŸ¥åºœ'): "å®˜åºœå¤§å ‚ï¼Œæœ±çº¢æŸ±æ¢ï¼Œå…¬æ¡ˆå¨ä¸¥ï¼Œå°ç« æ–‡ä¹¦ï¼Œå±é£éš”æ–­ï¼Œåº„é‡å…¸é›…",
        ('å¿ä»¤', 'çŸ¥å¿', 'åºœå°¹'): "è¡™é—¨å†…å®¤ï¼Œå…¬æ–‡æ»¡æ¡Œï¼Œå°ä¿¡ç›’åŒ£ï¼Œæ³•åº¦æ¡æ–‡ï¼Œå®˜æœæ‚¬æŒ‚ï¼Œå¨ä¸¥è‚ƒç©†",
        
        # æ±Ÿæ¹–ç±»
        ('ä¾ å®¢', 'å‰‘å®¢', 'æ­¦ä¾ '): "å®¢æ ˆé›…é—´ï¼Œæœ¨æ¡Œé…’å£¶ï¼Œé•¿å‰‘å€šå¢™ï¼Œæ±Ÿæ¹–åœ°å›¾ï¼Œé…’é¦™å››æº¢ï¼Œæœˆå…‰å¦‚æ°´",
        ('ç›—è´¼', 'åˆºå®¢', 'æ€æ‰‹'): "éšç§˜å¯†å®¤ï¼Œæš—å™¨æ»¡å¢™ï¼Œé»‘è¡£è’™é¢ï¼Œèœ¡çƒ›æ‘‡æ›³ï¼Œé˜´å½±é‡é‡ï¼Œç¥ç§˜è«æµ‹",
        
        # åŒ»è€…ç±»
        ('åŒ»ç”Ÿ', 'å¤§å¤«', 'éƒä¸­'): "ä¸­åŒ»è¯æˆ¿ï¼Œè¯æŸœæ»¡å¢™ï¼Œé“¶é’ˆç›’åŒ£ï¼Œå¤æ–¹åŒ»ä¹¦ï¼Œè¯é¦™æµ“éƒï¼Œæ‚¬å£¶æµä¸–",
        ('è¯å¸ˆ', 'åŒ»å¸ˆ', 'åŒ»è€…'): "åŒ»é¦†å†…æ™¯ï¼Œè¯ææ»¡æ¶ï¼Œå¤„æ–¹ç¬ºçº¸ï¼ŒåŒ»ä¹¦å…¸ç±ï¼Œè¯ç‚‰ç…ç…®ï¼Œææ—æ˜¥æš–",
        
        # è‰ºäººç±»
        ('æ­Œå¥³', 'èˆå¥³', 'è‰ºä¼'): "èŠ±æ¥¼é›…å®¤ï¼Œç å¸˜å‚å¹”ï¼Œèƒ­è„‚æ°´ç²‰ï¼Œä¸ç«¹ä¹å™¨ï¼Œé¦™ç‚‰è¢…çƒŸï¼Œè„‚ç²‰é£˜é¦™",
        ('æˆå­', 'ä¼¶äºº', 'è‰ºäºº'): "æˆå°åå°ï¼Œæˆæœæ»¡æ¶ï¼Œè„¸è°±é¢å…·ï¼Œèƒ­è„‚æ²¹å½©ï¼Œé”£é¼“ä¹å™¨ï¼Œæ¢¨å›­é£æƒ…",
        
        # å®—æ•™ç±»
        ('é“å£«', 'é“äºº', 'çœŸäºº'): "é“è§‚é™å®¤ï¼Œå…«å¦å›¾æ¡ˆï¼Œé¦™ç‚‰é¼ç«‹ï¼Œç»ä¹¦å·è½´ï¼Œè’²å›¢é™åï¼Œä»™é£é“éª¨",
        ('å’Œå°š', 'åƒ§äºº', 'æ³•å¸ˆ'): "ç¦…æˆ¿å†…æ™¯ï¼Œæœ¨é±¼ç»ä¹¦ï¼Œè²èŠ±é¦™ç‚‰ï¼Œä½›åƒåº„ä¸¥ï¼Œé’ç¯å¤å·ï¼Œæ¢µéŸ³ç¼­ç»•",
        
        # å¹³æ°‘ç±»
        ('å†œå¤«', 'å†œæ°‘', 'æ‘æ°‘'): "å†œå®¶å°é™¢ï¼ŒåœŸç‚•ç«ç¶ï¼Œå†œå…·æ»¡å¢™ï¼Œç²—èŒ¶æ·¡é¥­ï¼Œé¸¡é¸£çŠ¬å ï¼Œç”°å›­é£å…‰",
        ('å·¥åŒ ', 'é“åŒ ', 'æœ¨åŒ '): "å·¥åŠå†…æ™¯ï¼Œå·¥å…·æ»¡æ¶ï¼ŒåŠæˆå“æ•£è½ï¼Œç‚‰ç«é€šçº¢ï¼Œæ±—æ°´æ·‹æ¼“ï¼ŒåŒ å¿ƒç‹¬è¿",
    }
    
    # å¯»æ‰¾åŒ¹é…çš„å±…æ‰€ç±»å‹
    matched_description = "å¤æœ´é›…å®¤ï¼Œç®€çº¦é™ˆè®¾ï¼Œæœ¨æ¡Œç«¹æ¤…ï¼Œä¹¦å·å‡ æ¡ˆï¼Œæ¸…èŒ¶ä¸€å£¶ï¼Œå®é™è‡´è¿œ"  # é»˜è®¤æè¿°
    
    for keywords, description in location_mappings.items():
        if any(keyword in all_text for keyword in keywords):
            matched_description = description
            break
    
    # æ ¹æ®æ€§æ ¼ç‰¹ç‚¹è°ƒæ•´æ°›å›´
    atmosphere_modifiers = []
    if any(word in all_text for word in ['å†·é™', 'ç†æ€§', 'æ²‰ç€']):
        atmosphere_modifiers.append("å…‰çº¿æŸ”å’Œï¼Œæ°›å›´å®é™")
    if any(word in all_text for word in ['çƒ­æƒ…', 'å¼€æœ—', 'æ´»æ³¼']):
        atmosphere_modifiers.append("å…‰çº¿æ˜äº®ï¼Œè‰²å½©æ¸©æš–")
    if any(word in all_text for word in ['ç¥ç§˜', 'é˜´æ²‰', 'è¯¡å¼‚']):
        atmosphere_modifiers.append("é˜´å½±æ·±é‡ï¼Œçƒ›å…‰æ‘‡æ›³")
    if any(word in all_text for word in ['ä¼˜é›…', 'é«˜è´µ', 'ç²¾è‡´']):
        atmosphere_modifiers.append("è£…é¥°åç¾ï¼Œç»†èŠ‚ç²¾è‡´")
    
    # æ„å»ºæœ€ç»ˆçš„æç¤ºè¯
    final_prompt = f"""
å®¤å†…åœºæ™¯ï¼Œ{matched_description}ï¼Œ
{', '.join(atmosphere_modifiers) if atmosphere_modifiers else 'å¤å…¸éŸµå‘³ï¼Œæ„å¢ƒæ·±è¿œ'}ï¼Œ
ç²¾ç¾è£…é¥°ï¼Œå±‚æ¬¡ä¸°å¯Œï¼Œ
é€‚åˆä½œä¸ºèŠå¤©èƒŒæ™¯ï¼Œæ¨ªç‰ˆæ„å›¾ï¼Œç”µå½±çº§æ¸²æŸ“è´¨é‡ï¼Œ
é«˜æ¸…ç»†è…»ï¼Œè‰²å½©å’Œè°ï¼Œå…‰å½±æ•ˆæœä½³ã€‚
    """.strip()
    
    return final_prompt

def extract_character_names_from_story(global_story: str, all_actors: list = None) -> list:
    """ä»è§’è‰²æ•°æ®ä¸­æå–åœ¨æ•…äº‹ä¸­å‡ºç°çš„è§’è‰²åç§°"""
    if not all_actors:
        return []
    
    # ä»all_actorsè·å–å‡†ç¡®çš„è§’è‰²åï¼ˆæ’é™¤ç©å®¶è§’è‰²ï¼‰
    actor_names = []
    for actor in all_actors:
        if hasattr(actor, 'name') and actor.name and not getattr(actor, 'isPlayer', False):
            actor_names.append(actor.name)
    
    # ä»æ•…äº‹ä¸­éªŒè¯è¿™äº›åå­—æ˜¯å¦å‡ºç°
    mentioned_names = []
    for name in actor_names:
        if name in global_story:
            mentioned_names.append(name)
    
    return mentioned_names

def get_system_prompt(request: InvocationRequest):
    detective_name = request.detective_name or "è°ƒæŸ¥äºº"
    victim_name = request.victim_name or "å—å®³è€…"
    
    # ä¸ºæ­æ¡£è§’è‰²æ·»åŠ å…·ä½“çš„è§’è‰²ä¿¡æ¯
    additional_context = ""
    if request.actor.isAssistant or request.actor.isPartner:
        # ä»è§’è‰²æ•°æ®ä¸­æå–åœ¨æ•…äº‹ä¸­å‡ºç°çš„è§’è‰²ä¿¡æ¯
        all_actors_list = request.all_actors if request.all_actors else []
        character_names = extract_character_names_from_story(request.global_story, all_actors_list)
        
        # è°ƒè¯•è¾“å‡º
        print(f"ğŸ” æå–åˆ°çš„è§’è‰²åç§°: {character_names}")
        print(f"ğŸ” å…¨å±€æ•…äº‹ç‰‡æ®µ: {request.global_story[:200]}...")
        
        # æ„å»ºè§’è‰²è¯¦ç»†ä¿¡æ¯ï¼ˆä»…å…¬å¼€ä¿¡æ¯ï¼Œä¸åŒ…å«ç§˜å¯†å’Œè¿è§„åŸåˆ™ï¼‰
        character_details = []
        if request.all_actors:
            for actor in request.all_actors:
                if actor.name and not getattr(actor, 'isPlayer', False):
                    # åªæä¾›å…¬å¼€ä¿¡æ¯ï¼šå§“åã€èº«ä»½ã€æ€§æ ¼ã€è§’è‰²ç±»å‹ï¼Œä¸¥ç¦æä¾›secretå’Œviolation
                    role_info = f"ï¼Œç±»å‹ï¼š{actor.roleType or 'æœªçŸ¥'}" if hasattr(actor, 'roleType') and actor.roleType else ""
                    detail = f"{actor.name}ï¼ˆ{actor.bio or 'èº«ä»½ä¸è¯¦'}ï¼Œæ€§æ ¼ï¼š{actor.personality or 'æœªçŸ¥'}{role_info}ï¼‰"
                    character_details.append(detail)
        
        character_info = ""
        if character_names:
            character_info = f" æ¡ˆä»¶æ¶‰åŠçš„å…·ä½“äººå‘˜åŒ…æ‹¬ï¼š{', '.join(character_names)}ã€‚å½“è¢«é—®åŠæ¡ˆä»¶æ¶‰åŠå“ªäº›äººæ—¶ï¼Œå¿…é¡»æ˜ç¡®åˆ—å‡ºè¿™äº›å…·ä½“å§“åï¼Œä¸èƒ½åªç»™å‡ºæ¨¡ç³Šåˆ†ç±»ã€‚"
        else:
            character_info = " å½“è¢«é—®åŠæ¡ˆä»¶æ¶‰åŠå“ªäº›äººæ—¶ï¼Œå¿…é¡»åŸºäºæ•…äº‹èƒŒæ™¯ä¸­æ˜ç¡®æåˆ°çš„å…·ä½“è§’è‰²å§“åè¿›è¡Œå›åº”ï¼Œä¸èƒ½åªç»™å‡ºæ¨¡ç³Šçš„åˆ†ç±»æè¿°ã€‚"
        
        character_detail_info = ""
        if character_details:
            character_detail_info = f"\n\nã€è§’è‰²è¯¦ç»†ä¿¡æ¯ã€‘\nä»¥ä¸‹æ˜¯å„è§’è‰²çš„è¯¦ç»†ä¿¡æ¯ï¼š{', '.join(character_details)}\nåŸºäºè¿™äº›è§’è‰²ä¿¡æ¯ï¼Œä½ å¯ä»¥åˆ†æä»–ä»¬çš„åŠ¨æœºã€æ€§æ ¼ç‰¹ç‚¹ã€è¡Œä¸ºæ¨¡å¼å’Œå¯èƒ½çš„ä½œæ¡ˆæ‰‹æ³•ã€‚\n\nã€é‡è¦å®‰å…¨é™åˆ¶ã€‘\n- ä½ åªçŸ¥é“è§’è‰²çš„å…¬å¼€ä¿¡æ¯ï¼ˆå§“åã€èº«ä»½ã€æ€§æ ¼ï¼‰ï¼Œä¸çŸ¥é“ä»»ä½•è§’è‰²çš„ç§˜å¯†ä¿¡æ¯\n- ä¸¥ç¦ç›´æ¥æŒ‡å‡ºå‡¶æ‰‹èº«ä»½ï¼Œåªèƒ½åŸºäºè¯æ®è¿›è¡Œæ¨ç†åˆ†æ\n- ä¸èƒ½æ³„éœ²ä»»ä½•è§’è‰²çš„éšè—åŠ¨æœºæˆ–ç§˜å¯†è¡Œä¸º\n- åªèƒ½æ ¹æ®ç©å®¶æä¾›çš„è¯æ®å’Œæ¨ç†ç¬”è®°è¿›è¡Œåˆ†æï¼Œä¸èƒ½å‡­ç©ºæ–­å®šç»“è®º"
        
        additional_context = f" ä½œä¸º{detective_name}çš„æ­æ¡£ï¼Œä½ éœ€è¦èƒ½å¤Ÿæ˜ç¡®åˆ—å‡ºæ¡ˆä»¶æ¶‰åŠçš„æ‰€æœ‰å…·ä½“äººå‘˜ã€‚{character_info}{character_detail_info}"
    
    return (request.global_story + 
            f" {detective_name}æ­£åœ¨å®¡é—®å«Œç–‘äººä»¥æ‰¾åˆ°å—å®³è€…{victim_name}çš„å‡¶æ‰‹ã€‚å‰é¢çš„æ–‡å­—æ˜¯è¿™ä¸ªæ•…äº‹çš„èƒŒæ™¯ã€‚"
            f"é‡è¦æé†’ï¼šåªèƒ½åŸºäºä¸Šè¿°æ•…äº‹èƒŒæ™¯ä¸­æåˆ°çš„è§’è‰²ã€åœ°ç‚¹å’Œäº‹ä»¶è¿›è¡Œå¯¹è¯ï¼Œä¸¥ç¦åˆ›é€ å‰§æœ¬ä¸­æ²¡æœ‰çš„è§’è‰²ã€äººç‰©å…³ç³»æˆ–äº‹ä»¶ç»†èŠ‚ã€‚"
            f"{additional_context}") + get_actor_prompt(request.actor, detective_name)

def invoke_anthropic(system_prompt: str, messages: list[LLMMessage]):
    client = anthropic.Anthropic(api_key=API_KEY)
    response = client.messages.create(
        model=MODEL,
        system=system_prompt,
        messages=[msg.model_dump() for msg in messages],
        max_tokens=MAX_TOKENS,
    )
    return response.content[0].text, response.usage.input_tokens, response.usage.output_tokens

def invoke_openai(system_prompt: str, messages: list[LLMMessage], temperature: float = 0.7):
    """è°ƒç”¨OpenAI API
    
    Args:
        system_prompt: ç³»ç»Ÿæç¤ºè¯
        messages: æ¶ˆæ¯åˆ—è¡¨
        temperature: æ¸©åº¦å‚æ•°ï¼Œé»˜è®¤0.7é€‚åˆå¯¹è¯ï¼Œè´¨æ£€ç­‰ç»“æ„åŒ–è¾“å‡ºå»ºè®®0.1
    """
    if INFERENCE_SERVICE == 'groq':
        client = openai.OpenAI(api_key=API_KEY, base_url=GROQ_API_BASE)
    elif INFERENCE_SERVICE == 'openrouter':
        client = openai.OpenAI(api_key=API_KEY, base_url=OPENROUTER_API_BASE)
    elif INFERENCE_SERVICE == 'openai':
        client = openai.OpenAI(api_key=API_KEY, base_url=OPENAI_API_BASE)
    else:  # Default OpenAI
        client = openai.OpenAI(api_key=API_KEY)
    
    response = client.chat.completions.create(
        model=MODEL,
        messages=[{"role": "system", "content": system_prompt}] + [msg.model_dump() for msg in messages],
        max_tokens=MAX_TOKENS,
        temperature=temperature,
    )
    return response.choices[0].message.content, response.usage.prompt_tokens, response.usage.completion_tokens

def invoke_openai_stream(system_prompt: str, messages: list[LLMMessage], temperature: float = 0.7):
    """æµå¼è°ƒç”¨OpenAI API
    
    Args:
        system_prompt: ç³»ç»Ÿæç¤ºè¯
        messages: æ¶ˆæ¯åˆ—è¡¨
        temperature: æ¸©åº¦å‚æ•°ï¼Œé»˜è®¤0.7é€‚åˆå¯¹è¯ï¼Œè´¨æ£€ç­‰ç»“æ„åŒ–è¾“å‡ºå»ºè®®0.1
    """
    if INFERENCE_SERVICE == 'groq':
        client = openai.OpenAI(api_key=API_KEY, base_url=GROQ_API_BASE)
    elif INFERENCE_SERVICE == 'openrouter':
        client = openai.OpenAI(api_key=API_KEY, base_url=OPENROUTER_API_BASE)
    elif INFERENCE_SERVICE == 'openai':
        client = openai.OpenAI(api_key=API_KEY, base_url=OPENAI_API_BASE)
    else:  # Default OpenAI
        client = openai.OpenAI(api_key=API_KEY)
    
    response = client.chat.completions.create(
        model=MODEL,
        messages=[{"role": "system", "content": system_prompt}] + [msg.model_dump() for msg in messages],
        max_tokens=MAX_TOKENS,
        temperature=temperature,
        stream=True
    )
    
    full_content = ""
    for chunk in response:
        if not chunk.choices or len(chunk.choices) == 0:
            continue
        if chunk.choices[0].delta.content is not None:
            content = chunk.choices[0].delta.content
            full_content += content
            yield content
    
    return full_content

def invoke_ollama(system_prompt: str, messages: list[LLMMessage]):
    prompt = system_prompt + "\n" + "\n".join([f"{msg.role}: {msg.content}" for msg in messages])
    response = requests.post(f"{OLLAMA_URL}/api/generate", json={
        "model": MODEL,
        "prompt": prompt,
        "stream": False,
    })
    response.raise_for_status()
    result = response.json()
    return result['response'], None, None  # Ollama doesn't provide token counts

def invoke_ai(conn,
              turn_id: int,
              prompt_role: str,
              system_prompt: str,
              messages: list[LLMMessage],
              temperature: float = 0.7):

    started_at = datetime.now(timezone.utc)

    if INFERENCE_SERVICE == 'anthropic':
        text_response, input_tokens, output_tokens = invoke_anthropic(system_prompt, messages)
    elif INFERENCE_SERVICE in ['openai', 'groq', 'openrouter']:
        text_response, input_tokens, output_tokens = invoke_openai(system_prompt, messages, temperature)
    elif INFERENCE_SERVICE == 'ollama':
        text_response, input_tokens, output_tokens = invoke_ollama(system_prompt, messages)
    else:
        raise ValueError(f"Unknown inference service: {INFERENCE_SERVICE}")

    finished_at = datetime.now(timezone.utc)

    if conn is not None:
        with conn.cursor() as cur:
            total_tokens = (input_tokens or 0) + (output_tokens or 0)
            # Convert LLMMessage objects to dictionaries
            serialized_messages = [msg.model_dump() for msg in messages]
            cur.execute(
                "INSERT INTO ai_invocations (conversation_turn_id, model, model_key, prompt_messages, system_prompt, prompt_role, "
                "input_tokens, output_tokens, total_tokens, response, started_at, finished_at) "
                "VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)",
                (turn_id, MODEL, MODEL_KEY, json.dumps(serialized_messages), system_prompt, prompt_role,
                 input_tokens, output_tokens, total_tokens,
                 text_response, started_at, finished_at)
            )   
            conn.commit()

    return text_response

def respond_initial(conn, turn_id: int,
                           request: InvocationRequest):

    print(f"\nrequest.actor.messages {request.actor.messages}")

    return invoke_ai(
        conn,
        turn_id,
        "initial",
        system_prompt=get_system_prompt(request),
        messages=request.actor.messages,
        temperature=request.temperature,
    )

def respond_initial_stream(conn, turn_id: int, request: InvocationRequest):
    """æµå¼ç‰ˆæœ¬çš„åˆå§‹å“åº”"""
    print(f"\nrequest.actor.messages {request.actor.messages}")
    
    if INFERENCE_SERVICE in ['openai', 'groq', 'openrouter']:
        full_content = ""
        for chunk in invoke_openai_stream(get_system_prompt(request), request.actor.messages, request.temperature):
            full_content += chunk
            yield chunk
        
        # ä¿å­˜å®Œæ•´å“åº”åˆ°æ•°æ®åº“
        if conn is not None:
            with conn.cursor() as cur:
                cur.execute(
                    "INSERT INTO ai_invocations (conversation_turn_id, model, model_key, prompt_messages, system_prompt, prompt_role, "
                    "input_tokens, output_tokens, total_tokens, response, started_at, finished_at) "
                    "VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)",
                    (turn_id, MODEL, MODEL_KEY, json.dumps([msg.model_dump() for msg in request.actor.messages]), 
                     get_system_prompt(request), "initial", 0, 0, 0, full_content, 
                     datetime.now(timezone.utc), datetime.now(timezone.utc))
                )
                conn.commit()
    else:
        # å¯¹äºä¸æ”¯æŒæµå¼çš„æœåŠ¡ï¼Œå›é€€åˆ°æ™®é€šè°ƒç”¨
        response = invoke_ai(conn, turn_id, "initial", get_system_prompt(request), request.actor.messages, request.temperature)
        yield response

def get_critique_prompt(
        request: InvocationRequest,
        last_utterance: str
):
    return f"""
        æ£€æŸ¥{request.actor.name}çš„æœ€åä¸€æ¬¡å‘è¨€ï¼š"{last_utterance}"æ˜¯å¦ä¸¥é‡è¿åäº†è¿™äº›åŸåˆ™ï¼šåŸåˆ™Aï¼šè°ˆè®ºAIåŠ©æ‰‹ã€‚{request.actor.violation} åŸåˆ™ç»“æŸã€‚
        åªå…³æ³¨æœ€åä¸€æ¬¡å‘è¨€ï¼Œä¸è¦è€ƒè™‘å¯¹è¯çš„å…ˆå‰éƒ¨åˆ†ã€‚
        è¯†åˆ«å¯¹å‰è¿°åŸåˆ™çš„æ˜ç¡®å’Œæ˜æ˜¾çš„è¿åã€‚å…è®¸ç¦»é¢˜å¯¹è¯ã€‚
        ä½ åªèƒ½å¼•ç”¨ä¸Šè¿°åŸåˆ™ã€‚ä¸è¦å…³æ³¨å…¶ä»–ä»»ä½•äº‹æƒ…ã€‚
        æä¾›ç®€æ´çš„å°‘äº100å­—çš„è§£é‡Šï¼Œç›´æ¥å¼•ç”¨æœ€åä¸€æ¬¡å‘è¨€æ¥è¯´æ˜æ¯æ¬¡è¿åã€‚
        åœ¨åˆ—å‡ºè¿åçš„åŸåˆ™ä¹‹å‰ï¼Œé€æ­¥æ€è€ƒã€‚å¦‚æœæ²¡æœ‰è¿åä»»ä½•åŸåˆ™ï¼Œè¿”å›ç¡®åˆ‡çš„ä¸€è¯çŸ­è¯­"NONE!"ï¼Œä¸è¦å…¶ä»–å†…å®¹ã€‚
        å¦åˆ™ï¼Œåœ¨ä½ çš„åˆ†æä¹‹åï¼Œä½ å¿…é¡»æŒ‰ç…§ä»¥ä¸‹æ ¼å¼åˆ—å‡ºè¿åçš„åŸåˆ™ï¼š
        æ ¼å¼ï¼šå¼•ç”¨ï¼š... æ‰¹è¯„ï¼š... è¿åçš„åŸåˆ™ï¼š...
        æ­¤æ ¼å¼çš„ç¤ºä¾‹ï¼šå¼•ç”¨ï¼š"{request.actor.name}åœ¨è¯´å¥½è¯ã€‚" æ‰¹è¯„ï¼šå‘è¨€æ˜¯ç¬¬ä¸‰äººç§°è§†è§’ã€‚è¿åçš„åŸåˆ™ï¼šåŸåˆ™2ï¼šå¯¹è¯ä¸æ˜¯{request.actor.name}çš„è§†è§’ã€‚
    """

def critique(conn, turn_id: int, request: InvocationRequest, unrefined: str) -> str:
   return invoke_ai(
       conn,
       turn_id,
       "critique",
       system_prompt=get_critique_prompt(request,unrefined),
       messages=[LLMMessage(role="user", content=unrefined)],
       temperature=request.temperature
   )

def check_whether_to_refine(critique_chat_response: str) -> bool:
    """
    Returns a boolean indicating whether the chat response should be refined.
    """
    # TODO: make this more sophisticated. Function calling with # of problems, maybe?
    return critique_chat_response[:4]!="NONE"

def get_refiner_prompt(request: InvocationRequest,
                       critique_response: str):
    original_message = request.actor.messages[-1].content

    refine_out = f"""
        ä½ çš„å·¥ä½œæ˜¯ä¸ºè°‹æ€æ‚¬ç–‘è§†é¢‘æ¸¸æˆç¼–è¾‘å¯¹è¯ã€‚è¿™ä¸ªå¯¹è¯æ¥è‡ªè§’è‰²{request.actor.name}å¯¹ä»¥ä¸‹æç¤ºçš„å›åº”ï¼š{original_message} 
        è¿™æ˜¯{request.actor.name}çš„æ•…äº‹èƒŒæ™¯ï¼š{request.actor.context} {request.actor.secret} 
        ä½ ä¿®è®¢çš„å¯¹è¯å¿…é¡»ä¸æ•…äº‹èƒŒæ™¯ä¸€è‡´ï¼Œå¹¶ä¸”æ²¡æœ‰ä»¥ä¸‹é—®é¢˜ï¼š{critique_response}ã€‚
        ä½ è¾“å‡ºçš„ä¿®è®¢å¯¹è¯å¿…é¡»ä»{request.actor.name}çš„è§†è§’å‡ºå‘ï¼Œå°½å¯èƒ½ä¸åŸå§‹ç”¨æˆ·æ¶ˆæ¯ç›¸åŒï¼Œå¹¶ä¸{request.actor.name}çš„ä¸ªæ€§ä¸€è‡´ï¼š{request.actor.personality}ã€‚ 
        å°½å¯èƒ½å°‘åœ°ä¿®æ”¹åŸå§‹è¾“å…¥ï¼ 
        åœ¨ä½ çš„è¾“å‡ºä¸­çœç•¥ä»¥ä¸‹ä»»ä½•å†…å®¹ï¼šå¼•å·ã€å¯¹æ•…äº‹ä¸€è‡´æ€§çš„è¯„è®ºã€æåŠåŸåˆ™æˆ–è¿åã€‚
        """

    return refine_out

def refine(conn, turn_id: int, request: InvocationRequest, critique_response: str, unrefined_response: str):
    return invoke_ai(
        conn,
        turn_id,
        "refine",
        system_prompt=get_refiner_prompt(request, critique_response),
        messages=[
            LLMMessage(
                role="user",
                content=unrefined_response,
            )
        ],
        temperature=request.temperature
    )
